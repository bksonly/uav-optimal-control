# 最优控制推导：变分法、最大值原理和动态规划
本文写于系统学习张嗣瀛、高丽群编著的《现代控制理论》的第七章“最优控制”后。


## 序
我在之前的LQR、离散MPC的推导中，已经多次应用了这章中动态规划和最大值原理的思想，但是一直没有系统地学习并且厘清它们之间的关系。因此出于好奇心，借着毕设的机会，我系统地拜读了这一在数学上有些烦难的章节。虽然有一些变量的定义上存在混淆，对初学者明晰概念有一定障碍，但总体来说写的水平很高，超过国内大部分教材。

在学完之后我发现，在控制问题 $\dot x= f(x,u,t)$ 的约束下，将其视作泛函的问题去求解，颇有些不自然。泛函的变量是函数的映射关系，但是在控制问题的语境下，变量是输入序列 $u(t)$ ，经由约束作用到 $x(t)$ 上，即 $t \to x$的映射。因此如果用泛函的变分法去理解这个问题，多少有些不符合物理直觉。但在 $J'$ 的全微分计算中，如果不借助泛函的概念似乎又会有困难。

## 问题定义
时不变系统 $\dot x= f(x,u)$ ，初态 $x|_{t=t_0}=x_0$ ，终止时间 $T$ 固定，但终态 $x|_{t=T}=x_T$ 不固定，求输入序列 $u(t)$ 使得代价函数 $J=\phi(x_T) + \int _{t_0} ^{T} L(x,u) dt$ 最小。

> 注意：$x_T$ 表示终端状态，而不是 $T$ 时刻的状态。此处终态用 $x_T$ 而不是 $x(T)$ 表示是为了避免在终端时间 $T$ 不固定时取变分下，$\delta x$ 与 $\delta T$ 不相互独立，终端时间变为 $T+\delta T$而引起的符号混淆，**但对本问题没有影响**。

## 变分法以及伴随的最大值原理
类比普通函数的拉格朗日乘子法（无证明），引入拉格朗日乘子 $\lambda$ ，使代价函数变为：

$J'= \phi(x_T) +  \int _{t_0} ^{T} L(x,u)+ \lambda^T(f(x,u)-\dot x)  \ dt$

对 $\int _{t_0} ^{T}-\lambda ^T \dot x \ dt$ 的部分应用分步积分,并且用 $H(x,u)=L(x,u) + \lambda ^T f(x,u)$ ：

$J'= \phi(x_T) +  \int _{t_0} ^{T} [H(x,u)+ \dot\lambda^T  x]  \ dt -\lambda(T)^T x_T +\lambda(t_0)^T x_0$

对 $u$ 取变分 $\delta u$ ，则轨迹 $x$ 也得到变分 $\delta x$ 。

> 为什么全微分没有 $\delta \lambda$ ?
>  $\lambda$ 只在函数取到极值时有物理意义 ： $\lambda$ 只在代价函数里时没有任何约束，也没有意义，只有在约束函数对各变量的偏导为 $0$ 的方程成立时，才有一个确定的取值，而此时正是极值取到的时候。因此可以说，$\lambda$ 取决于最优的 $u$ ，而不取决于 $u$ 的变分。

于是泛函 $J'$ 的变分（此处类比全微分）为：

$\delta J'  =  (\frac{\partial \phi}{\partial x_T})^T \delta x_T -\lambda(T)^T \delta{x_T} + \int_{t_0}^{T}[(\frac{\partial H}{\partial x})^T \delta x + (\frac{\partial H}{\partial u})^T \delta u+ \dot \lambda^T \delta x] \ dt$

> 泛函的变分是在极值附近求取的，因此严谨的写法应当写成在 $[x_T^*  ,   x^* , u^*]$ 处取得极值 $J^{'*}$ ，随后的变分也应当写成 $\frac{\partial H}{\partial x} |_{x=x^*}$ 等，但为了符号的简洁与表意顺畅，这里没有这么写。


整理得到：

$\delta J' =(\frac{\partial \phi}{\partial x_T} - \lambda(T))^T \delta x_T + \int_{t_0}^{T}[(\frac{\partial H}{\partial x} + \dot \lambda)^T \delta x + (\frac{\partial H}{\partial u})^T \delta u] \ dt$

由于 $\delta x$ 和 $\delta u$ 的任意性

$\lambda(T)=\frac{\partial \phi}{\partial x_T}$

$\dot \lambda = -\frac{\partial H}{\partial x}$

若 $u$ 无限制，则变分法可以解决问题，$\frac{\partial H}{\partial u}=0$

否则，要用到最大值原理，$H$ 对 $u$ 取最小值，往往在边界取到。


## 动态规划
对于这一问题，还有应用动态规划思想进行求解。动态规划思想的正确性，即最优性原理在此不做展开，也就是不去证明贝尔曼方程的正确性，而是直接应用。

引入记号 $V(x,t)$ 来表示取到最优时的代价函数值，即

$V(x,t)=J(x^*,u^*)= \min_{u(t)\in U} J(x,u)$

$V(x,t)$ 可以理解成系统从 $t$ 时刻的状态 $x(t)$ 开始的最优代价。



$V(x(t),t)=\min_{u(t)\in U} [\phi(x(T))+ \int _{t} ^{T} L(x,u) \ dt]$

$=\min_{u(t)\in U} [\phi(x(T))+ \int _{t} ^{t + \Delta t} L(x,u) + + \int _{t + \Delta t} ^{T} L(x,u)\ dt]$

> 由于最优性原理是倒推的，那么对于一条最优轨线来说，无论从中间哪个状态开始计算的最优控制都与先前的计算结果相同，也就是说 $u$ 同解。

因此上式可以写作

$V(x(t),t)= V(x(t+\Delta t),t+\Delta t) +\min_{u(t)\in U}[\int _{t} ^{t + \Delta t}  L(x,u)\ dt]$

这也就是贝尔曼方程（Bellman Equation）

> 在取到最优的条件下，最优轨线是一条依据初态确定但尚未算出来的曲线，因此 $x,u$ 只是 $t$ 的函数，对 $t$ 不是求偏导而是导数。

对 $V(x(t+\Delta t),t+\Delta t)$ 泰勒展开

$V(x(t+\Delta t),t+\Delta t)=V(x(t),t) +(\frac{\partial V}{\partial x})^T \dot x \  \Delta t +\frac{\partial V}{\partial t} \  \Delta t +o(\Delta t)^2$

代入贝尔曼方程得

$-\min_{u(t)\in U}[\int _{t} ^{t + \Delta t}  L(x,u)\ dt]=(\frac{\partial V}{\partial x})^T \dot x \  \Delta t +\frac{\partial V}{\partial t} \  \Delta t +o(\Delta t)^2$

由中值定理

$\int _{t} ^{t + \Delta t}  L(x,u)\ dt=\Delta t \cdot L(x(t+\alpha \Delta t),u(t+\alpha\Delta t))$

> 注意，其实此处的写法是不严谨的，只有在最优轨线上，$x,u$ 才可以写作是 $t$ 的函数，否则 $u$ 作为输入不是任何变量的函数而取决于外部的决策者，而 $x$ 也将取决于输入 $u$ 。这么写的正确是由前面的 $\min_{u(t)\in U}$ 保证的。

代入等式两端消掉一个 $\Delta t$ ，并且有 $\Delta t \to 0$

$\min_{u(t)\in U}[L(x,u)] +(\frac{\partial V}{\partial x})^T \dot x  +\frac{\partial V}{\partial t} =0$

代入约束 $\dot x= f(x,u)$ ，并且假定取到最优轨线，用 $u^*$ 替代 $u$

$L(x,u^*) +(\frac{\partial V}{\partial x})^T f(x,u^*) +\frac{\partial V}{\partial t} =0$

可以猜测该偏微分方程非常难解，因此只存在理论上的意义。并且在中值定理处要求 $V(x,t)$ 有连续偏导数，这并不总是能满足。
